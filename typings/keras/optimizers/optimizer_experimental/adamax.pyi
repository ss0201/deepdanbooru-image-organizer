"""
This type stub file was generated by pyright.
"""

from keras.optimizers.optimizer_experimental import optimizer
from keras.utils import generic_utils
from tensorflow.python.util.tf_export import keras_export

"""Adamax optimizer implementation."""
@generic_utils.register_keras_serializable()
@keras_export("keras.optimizers.experimental.Adamax", v1=[])
class Adamax(optimizer.Optimizer):
    """Optimizer that implements the Adamax algorithm.

    Adamax, a variant of Adam based on the infinity norm, is a first-order
    gradient-based optimization method. Due to its capability of adjusting the
    learning rate based on data characteristics, it is suited to learn
    time-variant process, e.g., speech data with dynamically changed noise
    conditions. Default parameters follow those provided in the paper (see
    references below).

    Initialization:

    ```python
    m = 0  # Initialize initial 1st moment vector
    u = 0  # Initialize the exponentially weighted infinity norm
    t = 0  # Initialize timestep
    ```

    The update rule for parameter `w` with gradient `g` is described at the end
    of section 7.1 of the paper (see the referenece section):

    ```python
    t += 1
    m = beta1 * m + (1 - beta) * g
    u = max(beta2 * u, abs(g))
    current_lr = learning_rate / (1 - beta1 ** t)
    w = w - current_lr * m / (u + epsilon)
    ```

    Args:
      learning_rate: A `tf.Tensor`, floating point value, a schedule that is a
        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable
        that takes no arguments and returns the actual value to use. The
        learning rate. Defaults to 0.001.
      beta_1: A float value or a constant float tensor. The exponential decay
        rate for the 1st moment estimates.
      beta_2: A float value or a constant float tensor. The exponential decay
        rate for the exponentially weighted infinity norm.
      epsilon: A small constant for numerical stability.
      {{base_optimizer_keyword_args}}

    Reference:
      - [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)
    """
    def __init__(self, learning_rate=..., beta_1=..., beta_2=..., epsilon=..., clipnorm=..., clipvalue=..., global_clipnorm=..., use_ema=..., ema_momentum=..., ema_overwrite_frequency=..., jit_compile=..., name=..., **kwargs) -> None:
        ...
    
    def build(self, var_list): # -> None:
        """Initialize optimizer variables.

        Adamax optimizer has 2 types of variables: momentums (denoted as m),
        exponentially weighted infinity norm (denoted as u).

        Args:
          var_list: list of model variables to build Adamax variables on.
        """
        ...
    
    def update_step(self, gradient, variable): # -> None:
        """Update step given gradient and the associated model variable."""
        ...
    
    def get_config(self): # -> dict[str, Unknown]:
        ...
    


