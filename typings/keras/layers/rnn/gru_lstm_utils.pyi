"""
This type stub file was generated by pyright.
"""

"""Utilities used by both the GRU and LSTM classes."""
_FUNCTION_API_NAME_ATTRIBUTE = ...
_FUNCTION_DEVICE_ATTRIBUTE = ...
CPU_DEVICE_NAME = ...
GPU_DEVICE_NAME = ...
RUNTIME_UNKNOWN = ...
RUNTIME_CPU = ...
RUNTIME_GPU = ...
CUDNN_AVAILABLE_MSG = ...
CUDNN_NOT_AVAILABLE_MSG = ...
def use_new_gru_lstm_impl(): # -> Literal[False]:
    ...

class DefunWrapper:
    """A wrapper with no deep copy of the Defun in LSTM/GRU layer."""
    def __init__(self, time_major, go_backwards, layer_name) -> None:
        ...
    
    def __deepcopy__(self, memo): # -> Self@DefunWrapper:
        ...
    


def canonical_to_params(weights, biases, shape, transpose_weights=...):
    """Utility function convert variable to cuDNN compatible parameter.

    Note that Keras weights for kernels are different from the cuDNN format.
    Eg.:

    ```
      Keras                 cuDNN
      [[0, 1, 2],  <--->  [[0, 2, 4],
       [3, 4, 5]]          [1, 3, 5]]
    ```

    If the input weights need to be in a unified format, then set
    `transpose_weights=True` to convert the weights.

    Args:
      weights: list of weights for the individual kernels and recurrent kernels.
      biases: list of biases for individual gate.
      shape: the shape for the converted variables that will be feed to cuDNN.
      transpose_weights: boolean, whether to transpose the weights.

    Returns:
      The converted weights that can be feed to cuDNN ops as param.
    """
    ...

def is_sequence_right_padded(mask):
    """Check the mask tensor and see if it right padded.

    For cuDNN kernel, it uses the sequence length param to skip the tailing
    timestep. If the data is left padded, or not a strict right padding (has
    masked value in the middle of the sequence), then cuDNN kernel won't be work
    properly in those cases.

    Left padded data: [[False, False, True, True, True]].
    Right padded data: [[True, True, True, False, False]].
    Mixture of mask/unmasked data: [[True, False, True, False, False]].

    Note that for the mixed data example above, the actually data RNN should see
    are those 2 Trues (index 0 and 2), the index 1 False should be ignored and
    not pollute the internal states.

    Args:
      mask: the Boolean tensor with shape [batch, timestep]

    Returns:
      boolean scalar tensor, whether the mask is strictly right padded.
    """
    ...

def has_fully_masked_sequence(mask):
    ...

def is_cudnn_supported_inputs(mask, time_major):
    ...

def calculate_sequence_by_mask(mask, time_major):
    """Calculate the sequence length tensor (1-D) based on the masking tensor.

    The masking tensor is a 2D boolean tensor with shape [batch, timestep]. For
    any timestep that should be masked, the corresponding field will be False.
    Consider the following example:
      a = [[True, True, False, False],
           [True, True, True, False]]
    It is a (2, 4) tensor, and the corresponding sequence length result should
    be 1D tensor with value [2, 3]. Note that the masking tensor must be right
    padded that could be checked by, e.g., `is_sequence_right_padded()`.

    Args:
      mask: Boolean tensor with shape [batch, timestep] or [timestep, batch] if
        time_major=True.
      time_major: Boolean, which indicates whether the mask is time major or
        batch major.
    Returns:
      sequence_length: 1D int32 tensor.
    """
    ...

def generate_defun_backend(unique_api_name, preferred_device, func, supportive_attributes):
    ...

def get_context_device_type(): # -> None:
    """Parse the current context and return the device type, eg CPU/GPU."""
    ...

def runtime(runtime_name):
    ...

def read_variable_value(v):
    """Read the value of a variable if it is variable."""
    ...

def function_register(func, *args, **kwargs):
    """Register a specialization of a `Function` into the graph.

    This won't actually call the function with the inputs, and only put the
    function definition into graph. Register function with different input param
    will result into multiple version of functions registered in graph.

    Args:
      func: the `Function` instance that generated by a @defun
      *args: input arguments for the Python function.
      **kwargs: input keyword arguments for the Python function.

    Returns:
      a `ConcreteFunction` object specialized to inputs and execution context.

    Raises:
      ValueError: When the input function is not a defun wrapped python
        function.
    """
    ...

