"""
This type stub file was generated by pyright.
"""

import contextlib
from tensorflow.python.ops import lookup_ops, resource_variable_ops
from tensorflow.python.types import core

"""Various classes representing distributed values for PS."""
load_context = ...
class AggregatingVariable(resource_variable_ops.BaseResourceVariable, core.Tensor):
  """A wrapper around a variable that aggregates updates across replicas."""
  def __init__(self, strategy, v, aggregation) -> None:
    ...
  
  def __deepcopy__(self, memo): # -> Self@AggregatingVariable:
    """Perform a deepcopy of the `AggregatingVariable`.

    Unlike the deepcopy of a regular tf.Variable, this keeps the original
    strategy and devices of the `AggregatingVariable`.  To avoid confusion
    with the behavior of deepcopy on a regular `Variable` (which does
    copy into new devices), we only allow a deepcopy of a `AggregatingVariable`
    within its originating strategy scope.

    Args:
      memo: The memoization object for `deepcopy`.

    Returns:
      A deep copy of the current `AggregatingVariable`.

    Raises:
      RuntimeError: If trying to deepcopy into a different strategy.
    """
    ...
  
  def get(self): # -> Unknown:
    ...
  
  @property
  def distribute_strategy(self): # -> Unknown:
    ...
  
  def __getattr__(self, name): # -> Any:
    ...
  
  def assign_sub(self, *args, **kwargs): # -> Any:
    ...
  
  def assign_add(self, *args, **kwargs): # -> Any:
    ...
  
  def assign(self, *args, **kwargs): # -> Any:
    ...
  
  @property
  def initializer(self):
    ...
  
  def initialized_value(self):
    ...
  
  @property
  def initial_value(self):
    ...
  
  @property
  def op(self):
    ...
  
  def value(self):
    ...
  
  def read_value(self):
    ...
  
  def sparse_read(self, indices, name=...):
    ...
  
  def eval(self, session=...):
    ...
  
  @property
  def graph(self):
    ...
  
  @property
  def device(self):
    ...
  
  @property
  def shape(self):
    ...
  
  @property
  def aggregation(self): # -> Unknown:
    ...
  
  @property
  def synchronization(self):
    ...
  
  @property
  def name(self):
    ...
  
  @property
  def trainable(self):
    ...
  
  @property
  def dtype(self):
    ...
  
  def __add__(self, o):
    ...
  
  def __radd__(self, o):
    ...
  
  def __sub__(self, o):
    ...
  
  def __rsub__(self, o):
    ...
  
  def __mul__(self, o):
    ...
  
  def __rmul__(self, o):
    ...
  
  def __truediv__(self, o):
    ...
  
  def __rtruediv__(self, o):
    ...
  
  def __floordiv__(self, o):
    ...
  
  def __rfloordiv__(self, o):
    ...
  
  def __mod__(self, o):
    ...
  
  def __rmod__(self, o):
    ...
  
  def __lt__(self, o) -> bool:
    ...
  
  def __le__(self, o) -> bool:
    ...
  
  def __gt__(self, o) -> bool:
    ...
  
  def __ge__(self, o) -> bool:
    ...
  
  def __and__(self, o):
    ...
  
  def __rand__(self, o):
    ...
  
  def __or__(self, o):
    ...
  
  def __ror__(self, o):
    ...
  
  def __xor__(self, o):
    ...
  
  def __rxor__(self, o):
    ...
  
  def __getitem__(self, o):
    ...
  
  def __pow__(self, o, modulo=...): # -> Literal[1]:
    ...
  
  def __rpow__(self, o): # -> Literal[1]:
    ...
  
  def __invert__(self):
    ...
  
  def __neg__(self):
    ...
  
  def __abs__(self):
    ...
  
  def __div__(self, o): # -> _NotImplementedType:
    ...
  
  def __rdiv__(self, o): # -> _NotImplementedType:
    ...
  
  def __matmul__(self, o): # -> _NotImplementedType:
    ...
  
  def __rmatmul__(self, o): # -> _NotImplementedType:
    ...
  
  def __str__(self) -> str:
    ...
  
  def __repr__(self): # -> str:
    ...
  


class CachingVariable(resource_variable_ops.BaseResourceVariable, core.Tensor):
  """A wrapper around a variable that caches read value locally."""
  def __init__(self, v) -> None:
    ...
  
  def get(self): # -> Unknown:
    ...
  
  def __getattr__(self, name): # -> Any:
    ...
  
  def read_value(self): # -> defaultdict[Unknown, Unknown] | Any | list[Unknown] | ObjectProxy:
    ...
  
  def sparse_read(self, indices, name=...):
    ...
  
  def cached_read_value(self): # -> defaultdict[Unknown, Unknown] | Any | list[Unknown] | ObjectProxy:
    ...
  
  def assign_sub(self, *args, **kwargs):
    ...
  
  def assign_add(self, *args, **kwargs):
    ...
  
  def assign(self, *args, **kwargs):
    ...
  
  @property
  def initializer(self):
    ...
  
  def initialized_value(self):
    ...
  
  @property
  def initial_value(self):
    ...
  
  @property
  def op(self):
    ...
  
  def value(self): # -> defaultdict[Unknown, Unknown] | Any | list[Unknown] | ObjectProxy:
    ...
  
  def eval(self, session=...):
    ...
  
  @property
  def graph(self):
    ...
  
  @property
  def device(self):
    ...
  
  @property
  def shape(self):
    ...
  
  @property
  def synchronization(self):
    ...
  
  @property
  def name(self):
    ...
  
  @property
  def trainable(self):
    ...
  
  @property
  def dtype(self):
    ...
  
  @property
  def constraint(self):
    ...
  
  def __array__(self, dtype=...): # -> ndarray[Unknown, Unknown]:
    ...
  
  def __complex__(self): # -> complex:
    ...
  
  def __int__(self) -> int:
    ...
  
  def __float__(self): # -> float:
    ...
  
  def numpy(self): # -> Any:
    ...
  
  def __str__(self) -> str:
    ...
  
  def __repr__(self): # -> str:
    ...
  


class DistributedTable(lookup_ops.StaticHashTable):
  """A distributed StaticHashTable for ParameterServerStrategy.

  An instance of DistributedTable has copies of a StaticHashTable and its
  resource handle on the coordinator of each worker, created at the
  DistributedTable instance initialization time with initializers on each
  worker. Users can call methods on a DistributedTable as if it were a
  StaticHashTable, which leads to execution with the resource local to the
  consumer worker (or the coordinator, if calling from the coordinator). This
  implementation relies on the fact that the methods of StaticHashTable are
  queried with the resource handle (instead of the python object).

  Currently, at saving time, a DistributedTable is saved as a StaticHashTable on
  the coordinator, and restoring a DistributedTable from SavedModel is not
  supported.
  """
  def __init__(self, strategy, wrapped_creator) -> None:
    ...
  
  def __getattr__(self, attr): # -> ((*args: Unknown, **kwargs: Unknown) -> Unknown) | property | Any:
    ...
  
  def resource_handle_call_time_value(self): # -> tuple[() -> (Any | Unknown), TypeSpec]:
    """Returns a closure to run for a resource handle at call time and its spec.

    This function is called in self.resource_handle to create a placeholder
    which returns a resource handle on some worker or on the coordinator.
    """
    ...
  
  @property
  def resource_handle(self):
    ...
  
  @property
  def is_distributed_table(self): # -> Literal[True]:
    ...
  
  def __tf_experimental_restore_capture__(self, concrete_function, internal_capture):
    ...
  


_local_resource_restore_context = ...
def get_current_local_resource_restore_context(): # -> Any | None:
  ...

@contextlib.contextmanager
def with_local_resource_restore_context(instance): # -> Generator[None, None, None]:
  ...

class LocalResourceRestoreContext:
  """Class holding information of a distributed instance, e.g. StaticHashTable.

  Pairing use with context manager `with_local_resource_restore_context` allows
  operations under this context manager to conveniently gets information of a
  component of the `RestoredDistributedTable` (and other restored distributed
  `CapturableResource` if we're supporting their distribution in the future),
  instead of looking it up from the mapping of the worker-to-resource handle.
  This is especially useful when we know which instance the operations should
  execute with and the mapping is not available yet.
  """
  def __init__(self, instance) -> None:
    ...
  


class RestoredDistributedTable(DistributedTable):
  """A restored and distributed StaticHashTable for ParameterServerStrategy."""
  def resource_handle_call_time_value(self): # -> tuple[() -> (Any | Unknown), TypeSpec]:
    """Returns a closure to run for a resource handle at call time and its spec.

    This function is called in self.resource_handle to create a placeholder
    which returns a resource handle on some worker or on the coordinator.
    """
    ...
  
  def __setattr__(self, name, value):
    ...
  


