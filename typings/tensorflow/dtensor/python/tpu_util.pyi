"""
This type stub file was generated by pyright.
"""

from typing import Dict, List, Optional
from tensorflow.dtensor.python import layout as layout_lib
from tensorflow.python.util.tf_export import tf_export

"""TPU-specific utilities for DTensor."""
_INITIALIZED_TPU_SYSTEMS = ...
_MESH_DIM_X = ...
_TPU_DEVICE_TYPE = ...
_dtensor_device = ...
_tpu_topology = ...
_all_core_ids = ...
_all_core_locations = ...
class _CoreLocation:
  """Represents a TPU core's location in the mesh."""
  def __init__(self, x: int = ..., y: int = ..., z: int = ..., core: int = ...) -> None:
    ...
  
  def __eq__(self, other) -> bool:
    ...
  
  def __ne__(self, other) -> bool:
    ...
  
  def __hash__(self) -> int:
    ...
  
  def __repr__(self): # -> str:
    ...
  
  def to_list(self): # -> list[int]:
    ...
  


@tf_export("experimental.dtensor.shutdown_tpu_system", v1=[])
def dtensor_shutdown_tpu_system(): # -> None:
  """Shutdown TPU system."""
  ...

@tf_export("experimental.dtensor.initialize_tpu_system", v1=[])
def dtensor_initialize_tpu_system(enable_coordination_service=...): # -> None:
  """Initialize the TPU devices.

  This functions performs additional TPU related initialization after
  calling `dtensor.initialize_multi_client` to initialize multi-client DTensor.
  Refer to `dtensor.initialize_multi_client` for relevant environment
  variables that controls the initialization of multi-client DTensor.

  Args:
    enable_coordination_service: If true, enable distributed coordination
      service to make sure that workers know the devices on each other, a
      prerequisite for data transfer through cross-worker rendezvous.

  Raises:
    RuntimeError: If running inside a tf.function.
    NotFoundError: If no TPU devices found in eager mode.
  """
  ...

def create_tpu_mesh(mesh_dim_names: List[str], mesh_shape: List[int], mesh_name: str, ring_dims: Optional[int] = ..., ring_axes: Optional[List[str]] = ..., ring_bounds: Optional[List[int]] = ..., can_split_host_across_rings: bool = ..., build_ring_across_rings: bool = ..., rotate_ring_across_rings: bool = ...) -> layout_lib.Mesh:
  """Returns a TPU mesh optimized for AllReduce ring reductions.

  Only as many as leading axes specified by `ring_axes` as necessary will be
  used to build rings, as long as the subslice formed by these axes have enough
  cores to contain a ring of the required size. The leftover axes in `ring_axes`
  won't affect results.

  Args:
    mesh_dim_names: List of mesh dimension names.
    mesh_shape: Shape of the mesh.
    mesh_name: A unique name for the mesh. If empty, internally generate one.
    ring_dims: Optional; The number of leading (ring_dims > 0) or trailing
      (ring_dims < 0) mesh dimensions to build rings for. If unspecified, build
      rings for all but the first dimension.
    ring_axes: Optional; A permutation of ["x", "y", "z", "core"], specifying
      the order of TPU topology axes to build rings in. If unspecified, default
      to ["core", "x", "y", "z"].
    ring_bounds: Optional; The maximum number of devices on each axis, in the x,
      y, z, core order. If unspecified, default to physical topology limits.
    can_split_host_across_rings: Optional; If true, devices attached to the same
      host (i.e., DTensor client) may get assigned to different rings. Setting
      it to false may cause some combinations of arguments to be infeasible; see
      DeviceAssignmentTest.testCreateMesh[No]SplittingHosts* for examples.
    build_ring_across_rings: Optional; If true, also build a data-parallel ring
      across model-parallel rings. This ring could be strided.
    rotate_ring_across_rings: Optional; If true, build the data-parallel ring in
      column-major instead of row-major order.
  """
  ...

def get_device_ids(mesh: layout_lib.Mesh, client_id: Optional[int] = ...) -> List[int]:
  """Returns the device IDs of all TPU cores local to the given client.

  A device ID is a non-negative integer that uniquely identifies a device in the
  mesh. For example, for a 2x2 mesh ('x', 'y'), this function returns a
  permutation of [0, 1, 2, 3].

  Note that device IDs and device locations are equivalent. The former is a
  linearization of the latter along mesh dimensions.

  Args:
    mesh: A TPU mesh.
    client_id: Optional; A DTensor client ID. If empty, query this client.
  """
  ...

def get_device_locations(mesh: layout_lib.Mesh, client_id: Optional[int] = ...) -> List[Dict[str, int]]:
  """Returns the device locations of all TPU cores local to the given client.

  A device location is a dictionary from dimension names to indices on those
  dimensions. For example, for a 2x2 mesh ('x', 'y'), this function returns a
  permutation of this list:

    [{'x': 0, 'y': 0},
     {'x': 0, 'y': 1},
     {'x': 1, 'y': 0},
     {'x': 1, 'y': 1}].

  Note that device IDs and device locations are equivalent. The former is a
  linearization of the latter along mesh dimensions.

  Args:
    mesh: A TPU mesh.
    client_id: Optional; A DTensor client ID. If empty, query this client.
  """
  ...

